{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz+st5cS8BmXgexiZDMa2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHL-edu/undergradutate/blob/main/SimpleCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RPPU72J4akj"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, BatchNormalization, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 常量定义\n",
        "IMAGE_WIDTH = 256\n",
        "IMAGE_HEIGHT = 256\n",
        "CHANNELS = 3\n",
        "DATASET_SIZE = 1000\n",
        "\n",
        "# 定义路径常量\n",
        "LOAD_H5_PATH = Path('/content/drive/MyDrive/Colab Notebooks/person_train_394_White.h5')\n",
        "MODEL_SAVE_PATH = Path('/content/drive/MyDrive/Colab Notebooks/output')\n",
        "PROCESSED_OUTPUT_PATH = Path('/content/drive/MyDrive/Colab Notebooks/output')\n",
        "\n",
        "# 确保输出目录存在\n",
        "PROCESSED_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def create_improved_cnn(input_shape, output_size):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.3),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.4),\n",
        "        Flatten(),\n",
        "        Dense(1024, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(output_size)\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def setup_callbacks():\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True),\n",
        "        ModelCheckpoint(\n",
        "            filepath =str(MODEL_SAVE_PATH / 'best_model.keras'),\n",
        "            monitor='val_loss',                                             # 监控指标\n",
        "            verbose=1,                                                      # 打印日志信息\n",
        "            save_best_only=True,                                            # 只保留最优模型\n",
        "            save_weights_only=False,                                        # 是否仅保存权重而非整个模型\n",
        "            mode='auto',                                                     # 当监控指标越低越好时设为'min'\n",
        "            save_freq='epoch',                                              # 每轮结束后保存一次\n",
        "            initial_value_threshold=None\n",
        "        ),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, mode='min', verbose=1)\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_data(hdf5_path):\n",
        "    with h5py.File(hdf5_path, 'r') as f:\n",
        "        images = np.array(f['images'])\n",
        "        keypoints = np.array(f['keypoints'])[:, :, :2]\n",
        "    # 确保图像数据在 [0, 1] 范围内\n",
        "    images = images.astype('float32')\n",
        "    images = np.clip(images, 0, 1)  # 裁剪到 [0, 1]\n",
        "    keypoints = keypoints.astype('float32')\n",
        "    return images, keypoints\n",
        "\n",
        "\n",
        "def split_data(images, keypoints):\n",
        "    return train_test_split(images, keypoints, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_path):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Train MAE')\n",
        "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "    plt.title('Training vs Validation MAE')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_keypoint_comparison(images, true_kps, pred_kps, indices, save_path):\n",
        "    n_samples = len(indices)\n",
        "    plt.figure(figsize=(5 * n_samples, 10))\n",
        "    for i, idx in enumerate(indices):\n",
        "        # 确保图像数据在 [0, 1] 范围内\n",
        "        img = images[idx]\n",
        "        img = np.clip(img, 0, 1)  # 再次裁剪以确保安全\n",
        "\n",
        "        # 真实关键点\n",
        "        plt.subplot(2, n_samples, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.scatter(true_kps[idx, :, 0] * IMAGE_WIDTH, true_kps[idx, :, 1] * IMAGE_HEIGHT,\n",
        "                    c='red', label='True', s=10)\n",
        "        plt.title(f'Sample {idx} - True')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # 预测关键点\n",
        "        plt.subplot(2, n_samples, i + 1 + n_samples)\n",
        "        plt.imshow(img)\n",
        "        pred_kps_reshaped = pred_kps[idx].reshape(-1, 2)\n",
        "        plt.scatter(pred_kps_reshaped[:, 0] * IMAGE_WIDTH, pred_kps_reshaped[:, 1] * IMAGE_HEIGHT,\n",
        "                    c='blue', label='Predicted', s=10)\n",
        "        plt.title(f'Sample {idx} - Predicted')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_error_distribution(true_kps, pred_kps, save_path):\n",
        "    errors = np.abs(true_kps.reshape(true_kps.shape[0], -1, 2) -\n",
        "                    pred_kps.reshape(pred_kps.shape[0], -1, 2))\n",
        "    errors = np.sqrt(np.sum(errors ** 2, axis=2))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(errors.shape[1]):\n",
        "        sns.kdeplot(errors[:, i], label=f'Keypoint {i + 1}')\n",
        "    plt.title('Keypoint Prediction Error Distribution')\n",
        "    plt.xlabel('Euclidean Error (pixels)')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_error_heatmap(true_kps, pred_kps, save_path):\n",
        "    errors = np.abs(true_kps.reshape(true_kps.shape[0], -1, 2) -\n",
        "                    pred_kps.reshape(pred_kps.shape[0], -1, 2))\n",
        "    errors = np.sqrt(np.sum(errors ** 2, axis=2))\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(errors.T, cmap='viridis', cbar_kws={'label': 'Error (pixels)'})\n",
        "    plt.title('Keypoint Prediction Error Heatmap')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Keypoint Index')\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 加载数据\n",
        "    hdf5_path = LOAD_H5_PATH\n",
        "    images, keypoints = load_data(hdf5_path)\n",
        "\n",
        "    # 检查数据范围（用于调试）\n",
        "    print(f\"Image data range: min={images.min():.4f}, max={images.max():.4f}\")\n",
        "\n",
        "    # 分割数据\n",
        "    X_train, X_val, y_train, y_val = split_data(images, keypoints)\n",
        "\n",
        "    # 创建模型\n",
        "    output_size = y_train.shape[1] * y_train.shape[2]\n",
        "    model = create_improved_cnn((IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS), output_size)\n",
        "    model.summary()\n",
        "\n",
        "    # 训练模型\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train.reshape(y_train.shape[0], -1),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val.reshape(y_val.shape[0], -1)),\n",
        "        callbacks=setup_callbacks(),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 评估模型\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val.reshape(y_val.shape[0], -1))\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}')\n",
        "\n",
        "    # 保存模型\n",
        "    model_save_path = MODEL_SAVE_PATH / f'person_model_{DATASET_SIZE}.keras'\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # 绘制训练历史\n",
        "    history_plot_path = PROCESSED_OUTPUT_PATH / f'training_history_{DATASET_SIZE}.png'\n",
        "    plot_training_history(history, history_plot_path)\n",
        "\n",
        "    # 预测\n",
        "    predictions = model.predict(X_val)\n",
        "\n",
        "    # 绘制多样本关键点对比（显示4个样本）\n",
        "    indices = [0, 10, 20, 30]\n",
        "    kp_comparison_path = PROCESSED_OUTPUT_PATH / f'keypoint_comparison_{DATASET_SIZE}.png'\n",
        "    plot_keypoint_comparison(X_val, y_val, predictions, indices, kp_comparison_path)\n",
        "\n",
        "    # 绘制误差分布\n",
        "    error_dist_path = PROCESSED_OUTPUT_PATH / f'error_distribution_{DATASET_SIZE}.png'\n",
        "    plot_error_distribution(y_val, predictions, error_dist_path)\n",
        "\n",
        "    # 绘制误差热图\n",
        "    error_heatmap_path = PROCESSED_OUTPUT_PATH / f'error_heatmap_{DATASET_SIZE}.png'\n",
        "    plot_error_heatmap(y_val, predictions, error_heatmap_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# 挂载 Google Drive 到 Colab 环境\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 定义路径配置\n",
        "CONFIG = {\n",
        "    'base_path': '/content/drive/MyDrive/Colab Notebooks',\n",
        "    'input_file': 'data/person_train_1000_White.h5',\n",
        "    'output_dir': 'RESoutput',\n",
        "    'subdirs': ['processoutput', 'output_Processed']\n",
        "}\n",
        "\n",
        "# 创建路径\n",
        "SAVE_PATH = os.path.join(CONFIG['base_path'], CONFIG['output_dir'])\n",
        "INPUT_PATH = os.path.join(CONFIG['base_path'], CONFIG['input_file'])\n",
        "\n",
        "# 清空 output_Processed 目录\n",
        "output_processed_path = os.path.join(SAVE_PATH, 'output_Processed')\n",
        "if os.path.exists(output_processed_path):\n",
        "    shutil.rmtree(output_processed_path)\n",
        "    print(f\"已清空目录: {output_processed_path}\")\n",
        "os.makedirs(output_processed_path, exist_ok=True)\n",
        "\n",
        "# 创建或保留其他子目录\n",
        "for subdir in CONFIG['subdirs']:\n",
        "    if subdir != 'output_Processed':  # 已处理 output_Processed\n",
        "        os.makedirs(os.path.join(SAVE_PATH, subdir), exist_ok=True)\n",
        "\n",
        "# 图像尺寸\n",
        "resize_width = 256\n",
        "resize_height = 256\n",
        "\n",
        "def create_model(input_shape, output_size):\n",
        "    \"\"\"创建 ResNet50 模型用于关键点预测\"\"\"\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False  # 冻结预训练层\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    predictions = Dense(output_size, activation='linear')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def load_and_prepare_data(hdf5_path):\n",
        "    \"\"\"加载并准备数据\"\"\"\n",
        "    with h5py.File(hdf5_path, 'r') as f:\n",
        "        images = np.array(f['images'])\n",
        "        keypoints = np.array(f['keypoints'])[:, :, :2]  # 提取 x, y 坐标\n",
        "    X_train, X_val, y_train, y_val = train_test_split(images, keypoints, test_size=0.2, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def plot_predictions(images, keypoints, predictions, indices, save_path):\n",
        "    \"\"\"绘制预测与真实关键点的对比图\"\"\"\n",
        "    for idx in indices:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        # 真实关键点\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(images[idx])\n",
        "        plt.scatter(keypoints[idx, :, 0], keypoints[idx, :, 1], c='red', s=20, label='Ground Truth')\n",
        "        plt.title('Ground Truth')\n",
        "        plt.legend()\n",
        "        plt.axis('off')\n",
        "        # 预测关键点\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(images[idx])\n",
        "        pred_keypoints = predictions[idx].reshape(-1, 2)\n",
        "        plt.scatter(pred_keypoints[:, 0], pred_keypoints[:, 1], c='blue', s=20, label='Predictions')\n",
        "        plt.title('Predictions')\n",
        "        plt.legend()\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        # 保存对比图\n",
        "        plt.savefig(os.path.join(save_path, f'prediction_comparison_{idx}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "def plot_error_analysis(keypoints, predictions, save_path):\n",
        "    \"\"\"绘制关键点预测的误差分析图\"\"\"\n",
        "    errors = np.abs(keypoints - predictions).reshape(-1, 2)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot([errors[:, 0], errors[:, 1]], labels=['X Error', 'Y Error'])\n",
        "    plt.title('Keypoint Prediction Error Distribution')\n",
        "    plt.ylabel('Absolute Error (pixels)')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_path, 'error_analysis.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def save_training_log(history, save_path):\n",
        "    \"\"\"保存训练日志为 CSV 文件\"\"\"\n",
        "    log_df = pd.DataFrame({\n",
        "        'epoch': range(1, len(history.history['loss']) + 1),\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'train_mae': history.history['mae'],\n",
        "        'val_mae': history.history['val_mae']\n",
        "    })\n",
        "    log_df.to_csv(os.path.join(save_path, 'training_log.csv'), index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 加载数据\n",
        "    X_train, X_val, y_train, y_val = load_and_prepare_data(INPUT_PATH)\n",
        "\n",
        "    # 归一化数据\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_val = X_val.astype('float32') / 255.0\n",
        "    y_train = y_train.astype('float32') / np.array([resize_width, resize_height])\n",
        "    y_val = y_val.astype('float32') / np.array([resize_width, resize_height])\n",
        "\n",
        "    # 创建模型\n",
        "    output_size = y_train.shape[1] * y_train.shape[2]  # 关键点数量 * 2 (x, y)\n",
        "    model = create_model((resize_width, resize_height, 3), output_size)\n",
        "    model.summary()\n",
        "\n",
        "    # 定义回调\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
        "\n",
        "    # 训练模型\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train.reshape(y_train.shape[0], -1),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val.reshape(y_val.shape[0], -1)),\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # 评估模型\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val.reshape(y_val.shape[0], -1))\n",
        "    print(f'验证损失: {val_loss:.4f}, 验证 MAE: {val_mae:.4f}')\n",
        "\n",
        "    # 预测并还原坐标\n",
        "    predictions = model.predict(X_val)\n",
        "    predictions = predictions.reshape(-1, y_val.shape[1], y_val.shape[2]) * np.array([resize_width, resize_height])\n",
        "    y_val_orig = y_val * np.array([resize_width, resize_height])\n",
        "    X_val_orig = X_val * 255.0  # 还原图像像素值以便可视化\n",
        "\n",
        "    # 可视化多个样本的预测结果\n",
        "    sample_indices = np.random.choice(X_val.shape[0], 5, replace=False)\n",
        "    plot_predictions(X_val_orig, y_val_orig, predictions, sample_indices, os.path.join(SAVE_PATH, 'output_Processed'))\n",
        "\n",
        "    # 误差分析\n",
        "    plot_error_analysis(y_val_orig, predictions, os.path.join(SAVE_PATH, 'output_Processed'))\n",
        "\n",
        "    # 保存训练日志\n",
        "    save_training_log(history, os.path.join(SAVE_PATH, 'output_Processed'))\n",
        "\n",
        "    # 绘制并保存训练指标图\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='训练损失')\n",
        "    plt.plot(history.history['val_loss'], label='验证损失')\n",
        "    plt.title('训练与验证损失')\n",
        "    plt.xlabel('轮次')\n",
        "    plt.ylabel('损失')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='训练 MAE')\n",
        "    plt.plot(history.history['val_mae'], label='验证 MAE')\n",
        "    plt.title('训练与验证 MAE')\n",
        "    plt.xlabel('轮次')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_PATH, 'output_Processed', 'loss_mae_TrainRes_1000.png'), dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "xpk_azjuSfkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import timm\n",
        "\n",
        "# 挂载 Google Drive 到 Colab 环境\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 定义路径配置\n",
        "CONFIG = {\n",
        "    'base_path': '/content/drive/MyDrive/Colab Notebooks',\n",
        "    'input_file': 'data/person_train_1000_White.h5',\n",
        "    'output_dir': 'RESoutput',\n",
        "    'subdirs': ['processoutput', 'output_Processed'],\n",
        "    'model_weights': 'models/pose_hrnet_w32_256x192.h5'  # 假设转换后的 TensorFlow 权重路径\n",
        "}\n",
        "\n",
        "# 创建路径\n",
        "SAVE_PATH = os.path.join(CONFIG['base_path'], CONFIG['output_dir'])\n",
        "INPUT_PATH = os.path.join(CONFIG['base_path'], CONFIG['input_file'])\n",
        "WEIGHTS_PATH = os.path.join(CONFIG['base_path'], CONFIG['model_weights'])\n",
        "\n",
        "# 清空 output_Processed 目录\n",
        "output_processed_path = os.path.join(SAVE_PATH, 'output_Processed')\n",
        "if os.path.exists(output_processed_path):\n",
        "    shutil.rmtree(output_processed_path)\n",
        "    print(f\"已清空目录: {output_processed_path}\")\n",
        "os.makedirs(output_processed_path, exist_ok=True)\n",
        "\n",
        "# 创建或保留其他子目录\n",
        "for subdir in CONFIG['subdirs']:\n",
        "    if subdir != 'output_Processed':\n",
        "        os.makedirs(os.path.join(SAVE_PATH, subdir), exist_ok=True)\n",
        "\n",
        "# 图像尺寸\n",
        "resize_width = 256\n",
        "resize_height = 256\n",
        "\n",
        "def create_model(input_shape, output_size):\n",
        "    \"\"\"创建 HRNet-W32 模型，加载 COCO 预训练权重\"\"\"\n",
        "    # 使用 timm 加载 HRNet-W32 骨干（ImageNet 预训练）\n",
        "    base_model = timm.create_model('hrnet_w32', pretrained=True, num_classes=0)\n",
        "\n",
        "    # 获取骨干网络输出\n",
        "    x = base_model.forward_features(tf.keras.Input(shape=input_shape))\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # 添加全连接层\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    predictions = Dense(output_size, activation='linear')(x)\n",
        "\n",
        "    # 创建模型\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # 尝试加载 COCO 预训练权重（假设已转换为 .h5 格式）\n",
        "    if os.path.exists(WEIGHTS_PATH):\n",
        "        model.load_weights(WEIGHTS_PATH)\n",
        "        print(f\"已加载 COCO 预训练权重: {WEIGHTS_PATH}\")\n",
        "    else:\n",
        "        print(f\"警告: 未找到 COCO 预训练权重 {WEIGHTS_PATH}，使用 ImageNet 预训练权重\")\n",
        "\n",
        "    # 冻结骨干网络\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # 编译模型\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def load_and_prepare_data(hdf5_path):\n",
        "    \"\"\"加载并准备数据\"\"\"\n",
        "    with h5py.File(hdf5_path, 'r') as f:\n",
        "        images = np.array(f['images'])\n",
        "        keypoints = np.array(f['keypoints'])[:, :, :2]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(images, keypoints, test_size=0.2, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def plot_predictions(images, keypoints, predictions, indices, save_path):\n",
        "    \"\"\"绘制预测与真实关键点的对比图\"\"\"\n",
        "    for idx in indices:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(images[idx])\n",
        "        plt.scatter(keypoints[idx, :, 0], keypoints[idx, :, 1], c='red', s=20, label='Ground Truth')\n",
        "        plt.title('Ground Truth')\n",
        "        plt.legend()\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(images[idx])\n",
        "        pred_keypoints = predictions[idx].reshape(-1, 2)\n",
        "        plt.scatter(pred_keypoints[:, 0], pred_keypoints[:, 1], c='blue', s=20, label='Predictions')\n",
        "        plt.title('Predictions')\n",
        "        plt.legend()\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_path, f'prediction_comparison_{idx}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "def plot_error_analysis(keypoints, predictions, save_path):\n",
        "    \"\"\"绘制关键点预测的误差分析图\"\"\"\n",
        "    errors = np.abs(keypoints - predictions).reshape(-1, 2)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot([errors[:, 0], errors[:, 1]], labels=['X Error', 'Y Error'])\n",
        "    plt.title('Keypoint Prediction Error Distribution')\n",
        "    plt.ylabel('Absolute Error (pixels)')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_path, 'error_analysis.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def save_training_log(history, save_path):\n",
        "    \"\"\"保存训练日志为 CSV 文件\"\"\"\n",
        "    log_df = pd.DataFrame({\n",
        "        'epoch': range(1, len(history.history['loss']) + 1),\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'train_mae': history.history['mae'],\n",
        "        'val_mae': history.history['val_mae']\n",
        "    })\n",
        "    log_df.to_csv(os.path.join(save_path, 'training_log.csv'), index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 加载数据\n",
        "    X_train, X_val, y_train, y_val = load_and_prepare_data(INPUT_PATH)\n",
        "\n",
        "    # 归一化数据\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_val = X_val.astype('float32') / 255.0\n",
        "    y_train = y_train.astype('float32') / np.array([resize_width, resize_height])\n",
        "    y_val = y_val.astype('float32') / np.array([resize_width, resize_height])\n",
        "\n",
        "    # 创建模型\n",
        "    output_size = y_train.shape[1] * y_train.shape[2]\n",
        "    model = create_model((resize_width, resize_height, 3), output_size)\n",
        "    model.summary()\n",
        "\n",
        "    # 定义回调\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
        "\n",
        "    # 训练模型\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train.reshape(y_train.shape[0], -1),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val.reshape(y_val.shape[0], -1)),\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # 评估模型\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val.reshape(y_val.shape[0], -1))\n",
        "    print(f'验证损失: {val_loss:.4f}, 验证 MAE: {val_mae:.4f}')\n",
        "\n",
        "    # 预测并还原坐标\n",
        "    predictions = model.predict(X_val)\n",
        "    predictions = predictions.reshape(-1, y_val.shape[1], y_val.shape[2]) * np.array([resize_width, resize_height])\n",
        "    y_val_orig = y_val * np.array([resize_width, resize_height])\n",
        "    X_val_orig = X_val * 255.0\n",
        "\n",
        "    # 可视化多个样本的预测结果\n",
        "    sample_indices = np.random.choice(X_val.shape[0], 5, replace=False)\n",
        "    plot_predictions(X_val_orig, y_val_orig, predictions, sample_indices, output_processed_path)\n",
        "\n",
        "    # 误差分析\n",
        "    plot_error_analysis(y_val_orig, predictions, output_processed_path)\n",
        "\n",
        "    # 保存训练日志\n",
        "    save_training_log(history, output_processed_path)\n",
        "\n",
        "    # 绘制并保存训练指标图\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='训练损失')\n",
        "    plt.plot(history.history['val_loss'], label='验证损失')\n",
        "    plt.title('训练与验证损失')\n",
        "    plt.xlabel('轮次')\n",
        "    plt.ylabel('损失')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='训练 MAE')\n",
        "    plt.plot(history.history['val_mae'], label='验证 MAE')\n",
        "    plt.title('训练与验证 MAE')\n",
        "    plt.xlabel('轮次')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_processed_path, 'loss_mae_TrainHRNet_COCO_1000.png'), dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "OZcrBO4A2A9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}